{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import codecs\n",
    "\n",
    "def parse_pages(path='text16e6.evt'):\n",
    "  '''\n",
    "  Return a mapping from single character representation of transcriptor to\n",
    "  array of line strings.\n",
    "  @kwarg str path: the path to the voynich full text\n",
    "  @returns\n",
    "    [str] page_order: a list of the page keys in order\n",
    "    dict d: # d[annotator][page] = ['line_one', 'line_two']\n",
    "  '''\n",
    "  page_order = []\n",
    "  d = defaultdict(lambda: defaultdict(list))\n",
    "  with codecs.open('text16e6.evt', 'r', 'latin1') as f:\n",
    "    f = f.read()\n",
    "    for line_idx, line in enumerate(f.split('\\n')):\n",
    "      if not line.strip(): continue\n",
    "      if line[0] != '<': continue # skip paratextual lines\n",
    "      meta = line.split('<')[1].split('>')[0]\n",
    "      if '.' not in meta: # indicates the start of a new page (e.g. <f1r>)\n",
    "        page_order.append(meta)\n",
    "        continue \n",
    "      page, sheet, line_num_and_annotator = meta.split('.')\n",
    "      line_num, annotator = line_num_and_annotator.split(';')\n",
    "      if '>' not in line: continue\n",
    "      if not page: continue # skip the page id 0\n",
    "      line_text = line.split('>')[1].strip()\n",
    "      d[annotator][page].append(line_text)\n",
    "  return page_order, d\n",
    "      \n",
    "page_order, line_map = parse_pages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each key in line_map is a transcriber\n",
    "line_map.keys()\n",
    "\n",
    "# show how many pages each transcriber transcribed\n",
    "for k in line_map: print(k, len(line_map[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the annotator to use (Takahashi)\n",
    "annotator = 'H'\n",
    "\n",
    "# set page array\n",
    "pages = line_map[annotator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Voynichese Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_from_url(url, out_path):\n",
    "  '''Download a file at location `url` and write to `out_path`'''\n",
    "  if not os.path.exists(out_path):\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(out_path, 'wb').write(r.content)\n",
    "\n",
    "def download_voynichese_coords(page_id):\n",
    "  '''Download the page coords for `page_id` from voynichese.com'''\n",
    "  url = 'http://www.voynichese.com/2/data/folio/script/' + page_id + '.js'\n",
    "  download_from_url(url, join('voynichese', 'coords', page_id + '.js'))\n",
    "\n",
    "def download_voynichese_page(page_id):\n",
    "  '''Download a page image with page id `page_id` from voynichese.com'''\n",
    "  url = 'http://www.voynichese.com/2/data/folio/image/glance/color/large/' + page_id + '.jpg'\n",
    "  download_from_url(url, join('voynichese', 'images', page_id + '.jpg'))\n",
    "\n",
    "def download_voynichese_data(page_ids):\n",
    "  '''Download page images and word coords from voynichese.com'''\n",
    "  for i in ['coords', 'voynichese-images']:\n",
    "    if not os.path.exists(join('voynichese', i)):\n",
    "      os.makedirs(join('voynichese', i))\n",
    "  for page_id in page_ids:\n",
    "    download_voynichese_coords(page_id)\n",
    "    download_voynichese_page(page_id)\n",
    "\n",
    "download_voynichese_data(pages.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Biodiversity Heritage Library Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import flickr_api\n",
    "import os\n",
    "\n",
    "flickr_api.set_keys(api_key='a704ce9732b363a9caece2d65f7d041a', api_secret ='f3f5e1d5baaf4d38')\n",
    "if os.path.exists('flickr.credentials'):\n",
    "  flickr_api.set_auth_handler('flickr.credentials')\n",
    "else:\n",
    "  a = flickr_api.auth.AuthHandler() # creates a new AuthHandler object\n",
    "  perms = 'read' # set the required permissions\n",
    "  url = a.get_authorization_url(perms)\n",
    "  print(url) # open the printed url in a browser and agree; paste verifier code in xml response below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('flickr.credentials'):\n",
    "  a.set_verifier('5b58510bb6f0641b')\n",
    "  flickr_api.set_auth_handler(a)\n",
    "  a.save('flickr.credentials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Info(page=2, perpage=100, pages=1342, total=134143)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = flickr_api.Person.findByUserName('biodivlibrary')\n",
    "errored = []\n",
    "photos = user.getPhotos(page=2, perpage=100)\n",
    "photos.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if not os.path.exists('biodivlibrary-images'): os.makedirs('biodivlibrary-images')\n",
    "\n",
    "page_num = 6\n",
    "while page_num < photos.info.pages:\n",
    "  print(' * page num', page_num)\n",
    "  page_photos = user.getPhotos(page=page_num, perpage=100)  \n",
    "  for i in page_photos:\n",
    "    try:\n",
    "      out_path = join('biodivlibrary-images', i.id)\n",
    "      if os.path.exists(out_path): continue\n",
    "      i.save(out_path)\n",
    "      time.sleep(2)\n",
    "    except:\n",
    "      print(' * could not save', i)\n",
    "      errored.append(i)\n",
    "  page_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Smithsonian Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def get_page_images(page_number=0):\n",
    "  '''Save each herbal image in page number `page_number` to disk'''\n",
    "  print(' * fetching page', page_number)\n",
    "  r = requests.get('https://library.si.edu/topic/botany/images?page=' + str(page_number))\n",
    "  text = r.content.decode('utf8')\n",
    "  soup = BeautifulSoup(text)\n",
    "  imgs = soup.select('.dams-image')\n",
    "  for i in imgs:\n",
    "    try:\n",
    "      src = i.find('img')['src']\n",
    "      image_id = os.path.basename(src).split('id=')[1]\n",
    "      download_image(image_id)\n",
    "    except Exception as exc:\n",
    "      print(' ! err', i, exc)\n",
    "  if '/topic/botany/images?page=' + str(page_number+1) in text:\n",
    "    get_page_images(page_number+1)\n",
    "  \n",
    "def download_image(_id):\n",
    "  '''Download an image by SI image id'''\n",
    "  r = requests.get('https://ids.si.edu/ids/deliveryService?id=' + _id, allow_redirects=True)\n",
    "  open(os.path.join(out_dir, _id + '.jpg'), 'wb').write(r.content)\n",
    "\n",
    "# make output directory\n",
    "out_dir = 'smithsonian-images'\n",
    "if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
    "\n",
    "# fetch images\n",
    "get_page_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from skimage.transform import resize\n",
    "from skimage.util import montage\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil, floor\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import os, re, glob, json\n",
    "\n",
    "##\n",
    "# Show a single page image\n",
    "##\n",
    "\n",
    "def get_page_image(page_id):\n",
    "  '''Return a numpy array of a voynichese page image for `page_id`'''\n",
    "  return ndimage.imread(os.path.join('voynichese', 'images', page_id + '.jpg'))\n",
    "\n",
    "def show_page(page_id, figsize=(6, 14)):\n",
    "  '''Show the page image for a given page identifier (e.g.) f1r'''\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.imshow(get_page_image(page_id))\n",
    "\n",
    "##\n",
    "# Show frequency of string over course of text\n",
    "##\n",
    "  \n",
    "def plot_string_freq(s, figsize=(14, 1.4), clean=True):\n",
    "  '''Given a string `s` plot its distribution over pages'''\n",
    "  y = [get_page_string(i, clean=clean).count(s) for i in page_order]\n",
    "  x = list(range(len(page_order)))\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.bar(x, y, color='#9ab19d')\n",
    "  plt.title(s)\n",
    "\n",
    "def get_page_string(page_id, clean=True):\n",
    "  '''Return a string of the content from page `page_id` (e.g. f1r)'''\n",
    "  page_string = ' '.join(pages.get(page_id, []))\n",
    "  if clean: return clean_string(page_string)\n",
    "  return page_string\n",
    "\n",
    "def flatten(arr):\n",
    "  '''Flatten a 2d array to 1d'''\n",
    "  return [j for i in arr for j in i]\n",
    "\n",
    "def get_words(clean=True, unique=False):\n",
    "  '''Find the set of all words in the corpus'''\n",
    "  words = flatten([get_page_string(i, clean=clean).split('.') for i in page_order])\n",
    "  words = [i for i in words if i] # remove empty words\n",
    "  if unique: return set(words)\n",
    "  return words\n",
    "\n",
    "def clean_string(s):\n",
    "  '''Clean a voynich word string'''\n",
    "  s = s.replace('!', '').replace('%', '') # replace needless characters\n",
    "  s = s.replace('-', '.') # replace line breaks with word break (all line breaks assumed to be word breaks)\n",
    "  s = s.replace('=', '.') # replace end line comments with word break\n",
    "  s = s.replace(' ', '.') # replace whitespace with word break\n",
    "  s = re.sub(r'\\{[^\\{\\}]+?\\}', '', s) # drop content between {} github.com/viking-sudo-rm/voynich2vec/vms_tokenize.py\n",
    "  return s\n",
    "\n",
    "##\n",
    "# Show all occurrences of a word\n",
    "##\n",
    "\n",
    "def get_word_map():\n",
    "  '''\n",
    "  Find all occurrences of a word and return a map from:\n",
    "    d[word][page_id] = [numpy_array_of_word_image, numpy_array_of_word_image]\n",
    "  '''\n",
    "  word_map = defaultdict(lambda: defaultdict(list))\n",
    "  # Find all occurrences of a word\n",
    "  for page_id in pages:\n",
    "    page_image = get_page_image(page_id)\n",
    "    with open(join('voynichese', 'coords', page_id + '.js')) as f:\n",
    "      words, coords = json.load(f)\n",
    "      word_list = []\n",
    "      for word, _, _, _ in words:\n",
    "        word_list.append(word)\n",
    "      for word_idx, x, y, w, h in coords:\n",
    "        cropped = page_image[y:y+h,x:x+w]\n",
    "        if any([i == 0 for i in cropped.shape]): continue\n",
    "        word_map[word_list[word_idx]][page_id].append(cropped)\n",
    "  return word_map\n",
    "\n",
    "def resize_img(arr, size=(100, 20), anti_aliasing=False):\n",
    "  return resize(arr, size, anti_aliasing=anti_aliasing)\n",
    "\n",
    "def show_word_occurrences(word, figsize=(12, 8), grid_shape=None, grayscale=False, skip_verticals=True):\n",
    "  '''\n",
    "  Plot a montage of all instances of `word` in the voynich ms\n",
    "  @arg tuple figsize: the plot size\n",
    "  @arg tuple grid_shape: the number of rows, cols to include in the montage\n",
    "  @arg bool grayscale: whether to plot words in grayscale\n",
    "  @arg bool skip_verticals: skip words with vertical orientation\n",
    "  '''\n",
    "  imgs = flatten( [word_map.get(word, [])[page_id] for page_id in word_map.get(word, [])] )\n",
    "  if skip_verticals: imgs = [i for i in imgs if i.shape[0] < i.shape[1]]\n",
    "  if not imgs: raise Exception(' ! word has no images')\n",
    "  size = imgs[0].shape\n",
    "  resized = np.array([resize_img(i, size=size) for i in imgs])\n",
    "  if figsize: plt.figure(figsize=figsize)\n",
    "  composite = montage(resized, multichannel=True, grid_shape=grid_shape)\n",
    "  if grayscale:\n",
    "    plt.imshow(rgb2gray(composite), cmap=plt.cm.binary)\n",
    "  else:\n",
    "    plt.imshow(composite)\n",
    "\n",
    "##\n",
    "# Label each word occurrence\n",
    "##\n",
    "\n",
    "def label_word_occurrences(word):\n",
    "  '''Plot each occurrence of a word with a page and index label'''\n",
    "  for page_id in word_map.get(word, []):\n",
    "    for idx, i in enumerate(word_map['shedy'][page_id]):\n",
    "      plt.figure(figsize=(4,1.4))\n",
    "      plt.title(page_id + ' ' + str(idx))\n",
    "      plt.imshow(word_map['shedy'][page_id][idx])\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_page('f1v', figsize=(16,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = get_words()\n",
    "word_counts = Counter(words)\n",
    "word_map = get_word_map()\n",
    "\n",
    "for word, _ in word_counts.most_common(5):\n",
    "  plot_string_freq(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_word_occurrences('daiin', figsize=(30, 30), grayscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_word_occurrences('daiin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
