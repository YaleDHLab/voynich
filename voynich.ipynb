{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from skimage.transform import resize\n",
    "from skimage.util import montage\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil, floor\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import os, re, glob, json\n",
    "\n",
    "from helpers import pages\n",
    "\n",
    "##\n",
    "# Show a single page image\n",
    "##\n",
    "\n",
    "def get_page_image(page_id):\n",
    "  '''Return a numpy array of a voynichese page image for `page_id`'''\n",
    "  return ndimage.imread(os.path.join('voynichese', 'images', page_id + '.jpg'))\n",
    "\n",
    "def show_page(page_id, figsize=(6, 14)):\n",
    "  '''Show the page image for a given page identifier (e.g.) f1r'''\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.imshow(get_page_image(page_id))\n",
    "\n",
    "##\n",
    "# Show frequency of string over course of text\n",
    "##\n",
    "  \n",
    "def plot_string_freq(s, figsize=(14, 1.4), clean=True):\n",
    "  '''Given a string `s` plot its distribution over pages'''\n",
    "  y = [get_page_string(i, clean=clean).count(s) for i in page_order]\n",
    "  x = list(range(len(page_order)))\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.bar(x, y, color='#9ab19d')\n",
    "  plt.title(s)\n",
    "\n",
    "def get_page_string(page_id, clean=True):\n",
    "  '''Return a string of the content from page `page_id` (e.g. f1r)'''\n",
    "  page_string = ' '.join(pages.get(page_id, []))\n",
    "  if clean: return clean_string(page_string)\n",
    "  return page_string\n",
    "\n",
    "def flatten(arr):\n",
    "  '''Flatten a 2d array to 1d'''\n",
    "  return [j for i in arr for j in i]\n",
    "\n",
    "def get_words(clean=True, unique=False):\n",
    "  '''Find the set of all words in the corpus'''\n",
    "  words = flatten([get_page_string(i, clean=clean).split('.') for i in page_order])\n",
    "  words = [i for i in words if i] # remove empty words\n",
    "  if unique: return set(words)\n",
    "  return words\n",
    "\n",
    "def clean_string(s):\n",
    "  '''Clean a voynich word string'''\n",
    "  s = s.replace('!', '').replace('%', '') # replace needless characters\n",
    "  s = s.replace('-', '.') # replace line breaks with word break (all line breaks assumed to be word breaks)\n",
    "  s = s.replace('=', '.') # replace end line comments with word break\n",
    "  s = s.replace(' ', '.') # replace whitespace with word break\n",
    "  s = re.sub(r'\\{[^\\{\\}]+?\\}', '', s) # drop content between {} github.com/viking-sudo-rm/voynich2vec/vms_tokenize.py\n",
    "  return s\n",
    "\n",
    "##\n",
    "# Show all occurrences of a word\n",
    "##\n",
    "\n",
    "def get_word_map():\n",
    "  '''\n",
    "  Find all occurrences of a word and return a map from:\n",
    "    d[word][page_id] = [numpy_array_of_word_image, numpy_array_of_word_image]\n",
    "  '''\n",
    "  word_map = defaultdict(lambda: defaultdict(list))\n",
    "  # Find all occurrences of a word\n",
    "  for page_id in pages:\n",
    "    page_image = get_page_image(page_id)\n",
    "    with open(join('voynichese', 'coords', page_id + '.js')) as f:\n",
    "      words, coords = json.load(f)\n",
    "      word_list = []\n",
    "      for word, _, _, _ in words:\n",
    "        word_list.append(word)\n",
    "      for word_idx, x, y, w, h in coords:\n",
    "        cropped = page_image[y:y+h,x:x+w]\n",
    "        if any([i == 0 for i in cropped.shape]): continue\n",
    "        word_map[word_list[word_idx]][page_id].append(cropped)\n",
    "  return word_map\n",
    "\n",
    "def resize_img(arr, size=(100, 20), anti_aliasing=False):\n",
    "  return resize(arr, size, anti_aliasing=anti_aliasing)\n",
    "\n",
    "def show_word_occurrences(word, figsize=(12, 8), grid_shape=None, grayscale=False, skip_verticals=True):\n",
    "  '''\n",
    "  Plot a montage of all instances of `word` in the voynich ms\n",
    "  @arg tuple figsize: the plot size\n",
    "  @arg tuple grid_shape: the number of rows, cols to include in the montage\n",
    "  @arg bool grayscale: whether to plot words in grayscale\n",
    "  @arg bool skip_verticals: skip words with vertical orientation\n",
    "  '''\n",
    "  imgs = flatten( [word_map.get(word, [])[page_id] for page_id in word_map.get(word, [])] )\n",
    "  if skip_verticals: imgs = [i for i in imgs if i.shape[0] < i.shape[1]]\n",
    "  if not imgs: raise Exception(' ! word has no images')\n",
    "  size = imgs[0].shape\n",
    "  resized = np.array([resize_img(i, size=size) for i in imgs])\n",
    "  if figsize: plt.figure(figsize=figsize)\n",
    "  composite = montage(resized, multichannel=True, grid_shape=grid_shape)\n",
    "  if grayscale:\n",
    "    plt.imshow(rgb2gray(composite), cmap=plt.cm.binary)\n",
    "  else:\n",
    "    plt.imshow(composite)\n",
    "\n",
    "##\n",
    "# Label each word occurrence\n",
    "##\n",
    "\n",
    "def label_word_occurrences(word):\n",
    "  '''Plot each occurrence of a word with a page and index label'''\n",
    "  for page_id in word_map.get(word, []):\n",
    "    for idx, i in enumerate(word_map['shedy'][page_id]):\n",
    "      plt.figure(figsize=(4,1.4))\n",
    "      plt.title(page_id + ' ' + str(idx))\n",
    "      plt.imshow(word_map['shedy'][page_id][idx])\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_page('f1v', figsize=(16,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = get_words()\n",
    "word_counts = Counter(words)\n",
    "word_map = get_word_map()\n",
    "\n",
    "for word, _ in word_counts.most_common(5):\n",
    "  plot_string_freq(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_word_occurrences('daiin', figsize=(30, 30), grayscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_word_occurrences('daiin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
