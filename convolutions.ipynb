{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array, array_to_img\n",
    "from keras.applications import imagenet_utils, VGG19 #Xception, InceptionV3, MobileNetV2\n",
    "from uuid import NAMESPACE_URL, uuid1, uuid3\n",
    "from keras.models import load_model\n",
    "from skimage.color import rgb2gray\n",
    "from traceback import format_exc\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import lru_cache\n",
    "from annoy import AnnoyIndex\n",
    "from six import string_types\n",
    "from random import randint\n",
    "import keras.backend as K\n",
    "from copy import deepcopy\n",
    "from hashlib import sha1\n",
    "from sys import maxsize\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import skimage\n",
    "import glob, json, shears, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: This pipeline is founded on the idea that each image is composed of 0 or more \"figures\",\n",
    "where each figure is some semantic unit within the image (e.g. a single plant, or part of a plant).\n",
    "\n",
    "To clear all caches, remove `meta_d.json`, `./ann`, and `voynich.ann`.\n",
    "\n",
    "TODOS:\n",
    " * quantize npy vecs for faster i/o and reduced disk usage\n",
    " * suture individual metadata from the data repo to meta_d.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# NN MODEL\n",
    "##\n",
    "\n",
    "# VGG16, VGG19, and ResNet take 224×224 images; InceptionV3 and Xception take 299×299 inputs\n",
    "def resize_array(arr, shape=(304,304)):\n",
    "  '''Resize an array to a new shape'''\n",
    "  im = array_to_img(arr)\n",
    "  resized = im.resize(shape)\n",
    "  return img_to_array(resized)/255.0\n",
    "\n",
    "\n",
    "def get_uuid(*args, dtype='str', max_size=maxsize, right_pad=True):\n",
    "  '''Helper method that returns a random integer'''\n",
    "  if args and isinstance(args[0], string_types):\n",
    "    if dtype == 'int': i = str(get_int_hash(args[0]))\n",
    "    elif dtype == 'str': i = str(uuid3(NAMESPACE_URL, args[0]))\n",
    "  else:\n",
    "    if dtype == 'int': i = str(randint(0, max_size))\n",
    "    elif dtype == 'str': i = str(uuid1())\n",
    "  if dtype == 'int':\n",
    "    if right_pad:\n",
    "      while len(i) < len(str(max_size)): i = i + '0'\n",
    "    return int(i)\n",
    "  return i\n",
    "\n",
    "\n",
    "def get_int_hash(s):\n",
    "  '''Given a string return an integer hash of the string's content'''\n",
    "  try:\n",
    "    return int(sha1(args[0]).hexdigest(), 16)\n",
    "  except:\n",
    "    return int(sha1(args[0].encode('utf8')).hexdigest(), 16)\n",
    "\n",
    "\n",
    "def fit_transform(obj, sample_layer_index=-1):\n",
    "  # if the input array is empty, skip\n",
    "  if not np.any(obj): return False\n",
    "  if obj.shape == (0,): return None\n",
    "  # input shape must be n_images, h, w, colors: https://keras.io/preprocessing/image/\n",
    "  if hasattr(obj, 'shape') and len(obj.shape) == 3:\n",
    "    # this is an array of a single image\n",
    "    arr = resize_array(obj)\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "  elif hasattr(obj, 'shape') and len(obj.shape) == 4:\n",
    "    # this is an array of images\n",
    "    arr = np.array([resize_array(i) for i in obj])\n",
    "  else:\n",
    "    # this is assumed to be a single image\n",
    "    arr = resize_array(img_to_array(obj))/255.0 # see SO 47697622\n",
    "  # only Xception requires preprocessing\n",
    "  arr = imagenet_utils.preprocess_input(arr)\n",
    "  # extract the ith layer from the model (here, the -1th layer, or final layer)\n",
    "  out = K.function([model.input], [model.layers[sample_layer_index].output])([arr])\n",
    "  # parse out the k dim vector (k is determined by the size of the layer that's sampled from)\n",
    "  vec = out[0]\n",
    "  # return the vector to the calling scope\n",
    "  return vec\n",
    "\n",
    "\n",
    "##\n",
    "# IMAGE CLEANING\n",
    "##\n",
    "\n",
    "@lru_cache(maxsize=0)\n",
    "def load_image(path, color_mode='rgb'):\n",
    "  '''Given the path to an image return a keras image object'''\n",
    "  # nb all image loading should call this method rather than load_img\n",
    "  im = img_to_array(load_img(path, color_mode=color_mode))/255.0\n",
    "  if color_mode == 'grayscale': im = img_to_grayscale(im)\n",
    "  im = resize_image(im)\n",
    "  return im\n",
    "\n",
    "\n",
    "def resize_image(im, width=1024):\n",
    "  '''Given an image array, resize that image to a target width while maintaining aspect ratio'''\n",
    "  h,w = im.shape[:2]\n",
    "  height = h/w*width\n",
    "  resized = img_to_array( array_to_img(im).resize( (int(width), int(height) )) )/255.0\n",
    "  return resized\n",
    "  \n",
    "  \n",
    "def clean_image(im):\n",
    "  '''Given an image array return an image array that has standard width and non-significant pixels removed'''\n",
    "  im = resize_image(im)\n",
    "  _im = shears.remove_dominant_colors(im, mask_size=0.05, n_colors_to_remove=2)\n",
    "  _im = shears.filter_img(_im, min_size=1200, connectivity=60) # binarized mask-like\n",
    "  # create a mask from the remaining dark pixels in _im\n",
    "  im[np.where(_im==1)] = 1\n",
    "  return im\n",
    "\n",
    "\n",
    "##\n",
    "# IMAGE PARTITIONING\n",
    "##\n",
    "\n",
    "@lru_cache(maxsize=0)\n",
    "def get_image_figures(img_path, grayscale=True, resize_figures=True, min_area=10000):\n",
    "  '''\n",
    "  Given the path to an image, return an iterable of numpy arrays, with one\n",
    "  for each \"figure\" in the image, where a figure represents a single\n",
    "  object that's figured, or represented (like a plant, or planet)\n",
    "  ''' \n",
    "  orig = resize_image(load_image(img_path)) # get image\n",
    "  gray = skimage.color.rgb2gray(orig) # grayscale\n",
    "  o = np.zeros(gray.shape) # binarize\n",
    "  o[np.where(gray>skimage.filters.threshold_otsu(gray))] = 1 # binarize\n",
    "  o = shears.filter_img(o, min_size=500, connectivity=60) # remove text\n",
    "  o = 1-o # reverse figure / ground\n",
    "  l = [] # initialize list that will hold extracted figures\n",
    "  for i in skimage.measure.regionprops(skimage.measure.label(o, connectivity=2)): # find figures\n",
    "    if i.area >= min_area:\n",
    "      y_min, x_min, y_max, x_max = i.bbox\n",
    "      # quick gut check to make sure this patch is not abnormally \n",
    "      w = x_max-x_min\n",
    "      h = y_max-y_min\n",
    "      if (w/h<0.2) or (h/w<0.2): continue\n",
    "      img = img_to_grayscale(orig) if grayscale else orig\n",
    "      l.append(img[y_min:y_max, x_min:x_max])\n",
    "  if resize_figures:\n",
    "    return np.array([resize_array(i) for i in l])\n",
    "  return np.array([i for i in l])\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=0)\n",
    "def get_image_windows(img_path, color_mode='rgb'):\n",
    "  '''Given the path to an image, return an array of the image windows within that image'''\n",
    "  img = load_image(img_path, color_mode=color_mode)\n",
    "  arr = img_to_array(img)/255.0\n",
    "  windows = get_windows(arr)\n",
    "  return windows\n",
    "\n",
    "\n",
    "def get_windows(im, step=20, win=300, n_per_side=20, plot=False):\n",
    "  '''\n",
    "  Given a np array `im` with at least two dimensions, return windows of size (`win`, `win`).\n",
    "  To create the list of windows, remove text, binarize, then use that binarized result as a \n",
    "  mask to remove non-significant pixels from the original image. Finally, pass a sliding window\n",
    "  over the remainder. Only retain the subset of those windows that contain some minimum amount of\n",
    "  pixels in the mask.\n",
    "  '''\n",
    "  s = win+(step*n_per_side)\n",
    "  im = resize_array(im, shape=(s,s))\n",
    "  windows = []\n",
    "  dx = 0\n",
    "  dy = 0\n",
    "  for _ in range(im.shape[0]//step): # x axis pass\n",
    "    for _ in range(im.shape[1]//step): # y axis pass\n",
    "      w = im[dy:dy+win, dx:dx+win]\n",
    "      if w.shape == (win, win, 3):\n",
    "        windows.append(w)\n",
    "      if plot:\n",
    "        plt.imshow(w)\n",
    "        plt.title('{}-{}'.format(dx, dy))\n",
    "        plt.show()\n",
    "      dy += step\n",
    "    dx += step\n",
    "  return np.array(windows)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=0)\n",
    "def guid_to_vecs(guid):\n",
    "  '''Given a guid for a full image, get the array of vectors that corresponds to that image'''\n",
    "  vec_path = os.path.join(out_dir, '{}-vectors.npy'.format(guid))\n",
    "  try:\n",
    "    vecs = np.load(vec_path)\n",
    "    if np.any(vecs): return vecs\n",
    "  except:\n",
    "    print(' ! could not load vecs for guid', guid)\n",
    "  return []\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=0)\n",
    "def guid_to_vec(guid):\n",
    "  '''Given a guid for a figure, get the vector that corresponds to that figure'''\n",
    "  m = meta_d[guid]\n",
    "  return guid_to_vecs(m['parent_guid'])[m['vec_idx']]\n",
    "\n",
    "##\n",
    "# PLOTING\n",
    "##\n",
    "\n",
    "def plot_img_grid(img_list, rows=1, size_scalar=2, labels=[]):\n",
    "  '''Plot a list of images with `rows` rows'''\n",
    "  cols = ceil( len(img_list) / rows )\n",
    "  # initialize the plot\n",
    "  fig, axes = plt.subplots(rows, cols, figsize=(cols*size_scalar, rows*size_scalar), squeeze=False)\n",
    "  for idx, i in enumerate(img_list):\n",
    "    col = idx%cols\n",
    "    row = idx//cols\n",
    "    axes[row][col].imshow(img_list[idx])\n",
    "    if len(labels) > idx:\n",
    "      axes[row][col].set_title(labels[idx], fontsize=6.5)\n",
    "  plt.show()\n",
    "\n",
    "  \n",
    "def plot_guid(guid, title=None):\n",
    "  '''Given a GUID plot the image represented by that GUID'''\n",
    "  window = guid_to_img(guid)\n",
    "  title = title if title else guid_to_title(guid)\n",
    "  plt.imshow(window)\n",
    "  plt.title(title)\n",
    "  plt.show()\n",
    "  \n",
    "  \n",
    "def guid_to_img(guid):\n",
    "  '''Given a GUID return a numpy image'''\n",
    "  if meta_d['guid'].get('parent_guid', False):\n",
    "    parent_guid = meta_d['guid']['parent_guid']\n",
    "    windows = get_image_figures(meta_d[parent_guid]['path'])\n",
    "    return windows[guid][meta_d['guid']['vec_idx']]\n",
    "  else:\n",
    "    return img_to_arry(load_image(meta_d[guid]['path']))\n",
    "  \n",
    "  \n",
    "def guid_to_title(guid):\n",
    "  '''Given a GUID return a title for the image'''\n",
    "  m = meta_d[guid]\n",
    "  if not m.get('parent_guid', False):\n",
    "    return m.get('path')\n",
    "  return '{} {} {}'.format(m['collection'], m['idx_in_collection'], m['vec_idx'])\n",
    "\n",
    "\n",
    "def img_to_grayscale(im):\n",
    "  '''Given an image with shape h,w,3, return the image with the same shape in grayscale'''\n",
    "  im = rgb2gray(im).squeeze()\n",
    "  _im = np.zeros((im.shape[0], im.shape[1], 3))\n",
    "  # replicate identical color information across all channels\n",
    "  _im[:,:,0] = im\n",
    "  _im[:,:,1] = im\n",
    "  _im[:,:,2] = im\n",
    "  return _im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images\n",
    "\n",
    "Each image contains multiple figures. Extract each..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob('utils/voynichese/images/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images += glob.glob('data/uvm-italian-herbal/images/*.jpg') # load other collections..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for development & experimentation\n",
    "if False:\n",
    "  np.random.seed(0)\n",
    "  np.random.shuffle(images)\n",
    "  images = images[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = [get_image_figures(i) for i in images] # 2D list wherein sublists contain figures for the ith image in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(i) for i in figures]) # histogram of the number of figures in each image\n",
    "plt.title('Counts of Figures Per Input Image')\n",
    "plt.xlabel('n Figures')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things easy in subsequent sections, flatten each list of images and map each subimage to an index a-b, where a denotes index of the figure's parent image in all images, and b denotes the index of the figure within all figures in that image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_map = {}\n",
    "k = 0\n",
    "for idx, i in enumerate(figures):\n",
    "  for jdx, j in enumerate(i):\n",
    "    fig_map[k] = '{}-{}'.format(idx, jdx)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_color = np.array([j for i in figures for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# free up some memory\n",
    "del figures\n",
    "\n",
    "# optionally clear the cache on lru decorated functions\n",
    "load_image.cache_clear()\n",
    "get_image_figures.cache_clear()\n",
    "get_image_windows.cache_clear()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Vectorization\n",
    "\n",
    "Next let's transform each image into a vector. There are many methods we can use to achieve this:\n",
    "\n",
    "1. We can flatten each input image into a vector\n",
    "2. We can sample from a hidden layer in a pretrained network\n",
    "3. We can build our own network and vectorize images with that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Image Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = X = imgs_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize with Pretrained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16, VGG19, and ResNet take 224×224 images; InceptionV3 and Xception take 299×299 inputs\n",
    "X = np.array([resize_array(i, shape=(224,224)) for i in imgs_color])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the model to use and identify the size of its internal layers\n",
    "#model = Xception()\n",
    "#model = MobileNetV2()\n",
    "model = VGG19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(model.layers):\n",
    "  shape = i.output_shape[1:]\n",
    "  vals = np.prod(shape)\n",
    "  print(idx, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the index of the layer to use when vectorizing images\n",
    "# nb: different layers have different shapes!\n",
    "sample_layer_index = 6\n",
    "\n",
    "# only xception requires img preprocessing\n",
    "if model.name == 'xception': X = imagenet_utils.preprocess_input(X)\n",
    "  \n",
    "# vectorization function that extracts the output from the `sample_layer_index` layer\n",
    "vectorize = K.function([model.input], [model.layers[sample_layer_index].output])\n",
    "  \n",
    "# to process the entire dataframe in one shot\n",
    "if False:\n",
    "  z = vectorize(X)\n",
    "  z = np.array([i.flatten() for i in z])\n",
    "  \n",
    "# for progress\n",
    "if True:\n",
    "  vecs = []\n",
    "  for idx, i in enumerate(X):\n",
    "    query_img = np.expand_dims(i, 0)\n",
    "    vec = vectorize([query_img])[0]\n",
    "    vecs.append(vec)\n",
    "    print(' * vectorized', idx+1, 'of', len(X), 'frames')\n",
    "  z = np.array([i.flatten() for i in vecs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale X 0:1 so images can be visualized below\n",
    "X = rgb2gray(X)\n",
    "X = X-np.min(X)\n",
    "X /= np.max(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('z',z)\n",
    "np.save('X',X)\n",
    "np.save('images',images)\n",
    "json.dump(fig_map, open('fig_map.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize with a Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape, Dense, Flatten, Conv2D, MaxPooling2D, UpSampling2D\n",
    "\n",
    "conv1 = (6,6)\n",
    "conv2 = (5,5)\n",
    "conv3 = (4,4)\n",
    "conv4 = (4,4)\n",
    "\n",
    "pool1 = (2,2)\n",
    "pool2 = (2,2)\n",
    "pool3 = (2,2)\n",
    "pool4 = (2,2)\n",
    "\n",
    "class Autoencoder:\n",
    "  def __init__(self, img_shape=(304, 304, 1)):\n",
    "    if not img_shape: raise Exception('Please provide img_shape (height, width) in px')\n",
    "\n",
    "    # create the encoder\n",
    "    i = h = Input(img_shape) # the encoder takes as input images    \n",
    "    h = Conv2D(16, conv1, activation='relu', padding='same')(i)\n",
    "    h = MaxPooling2D(pool1, padding='same')(h)\n",
    "    h = Conv2D(8, conv2, activation='relu', padding='same')(h)\n",
    "    h = MaxPooling2D(pool2, padding='same')(h)\n",
    "    h = Conv2D(8, conv3, activation='relu', padding='same')(h)\n",
    "    h = MaxPooling2D(pool3, padding='same')(h)\n",
    "    h = Conv2D(4, conv4, activation='relu', padding='same')(h)\n",
    "    h = MaxPooling2D(pool4, padding='same')(h)\n",
    "    self.encoder = Model(inputs=[i], outputs=[h])\n",
    "\n",
    "    # create the decoder\n",
    "    i = h = Input((19, 19, 4))\n",
    "    h = Conv2D(4, conv4, activation='relu', padding='same')(h)\n",
    "    h = UpSampling2D(pool4)(h)\n",
    "    h = Conv2D(8, conv3, activation='relu', padding='same')(h)\n",
    "    h = UpSampling2D(pool3)(h)\n",
    "    h = Conv2D(8, conv2, activation='relu', padding='same')(h)\n",
    "    h = UpSampling2D(pool2)(h)\n",
    "    h = Conv2D(16, conv1, activation='relu', padding='same')(h)\n",
    "    h = UpSampling2D(pool1)(h)\n",
    "    h = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(h)\n",
    "    self.decoder = Model(inputs=[i], outputs=[h])\n",
    "\n",
    "    # combine the encoder and decoder into a full autoencoder\n",
    "    i = Input(img_shape) # take as input image vectors\n",
    "    z = self.encoder(i) # push observations into latent space\n",
    "    o = self.decoder(z) # project from latent space to feature space\n",
    "    self.model = Model(inputs=[i], outputs=[o])\n",
    "    self.model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# inspect the model\n",
    "autoencoder.encoder.summary()\n",
    "autoencoder.decoder.summary()\n",
    "autoencoder.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this autoencoder expects a single color channel (i.e. grayscale inputs)\n",
    "X = np.expand_dims(np.array([rgb2gray(i) for i in imgs_color]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the autoencoder - for production purposes run more epochs, ~100 is a decent start.\n",
    "# once the loss term ceases to fall much over many iterations,\n",
    "# you can lower the learning rate and train more...\n",
    "autoencoder.model.fit(X, X, batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, save or load a model -- useful if you run lots of training\n",
    "if False:\n",
    "  model.save('voynich.hdf5')\n",
    "if False:\n",
    "  model = load_model('voynich.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test autoencoder's reconstruction of sample inputs\n",
    "# the closer outputs are to the input, the better the model\n",
    "# has learned the data's features\n",
    "\n",
    "# select the index of the image to test with\n",
    "sample_idx = 131\n",
    "\n",
    "# plot the model input\n",
    "plt.imshow(X[sample_idx].squeeze())\n",
    "plt.show()\n",
    "\n",
    "# plot the model output\n",
    "o = autoencoder.model.predict(np.expand_dims(X[sample_idx], 0))\n",
    "plt.imshow(o.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify KNN\n",
    "\n",
    "Whichever vectorization method we use, let's now identify the k-nearest neighbors for each of our figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape, z[0].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a KNN index\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "build = True\n",
    "\n",
    "if build:\n",
    "  t = AnnoyIndex(len(z[0].flatten()), 'angular')\n",
    "  for idx, i in enumerate(z):\n",
    "    t.add_item(idx, i.flatten())\n",
    "  _ = t.build(1000)\n",
    "  t.save('voynich.ann')\n",
    "  \n",
    "else:\n",
    "  t = AnnoyIndex(len(z[0].flatten()), 'angular')\n",
    "  model = t.load('test.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nearest neighbors to find (the first knn is the query image itself usually)\n",
    "k = 4\n",
    "\n",
    "for i in range(len(z)):\n",
    "  knn, dist = t.get_nns_by_item(i, k, include_distances=True)\n",
    "  sims = [round(1-i, 4) for i in dist]\n",
    "  if max(sims[1:]) < 0.8: continue\n",
    "  img_list = [X[knn[i]].squeeze() for i in range(len(knn))] # show the most similar image to the input img\n",
    "  # curate titles for each match\n",
    "  titles = []\n",
    "  img_idx, fig_idx = zip(*[[int(j) for j in fig_map[i].split('-')] for i in knn])  \n",
    "  print('\\n'.join(['{} {} {}'.format(knn[idx], sims[idx], images[i]) for idx, i in enumerate(img_idx)]))\n",
    "  plot_img_grid(img_list, labels=sims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
