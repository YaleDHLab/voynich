{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array, array_to_img\n",
    "from keras.applications import Xception, VGG19, InceptionV3, imagenet_utils\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from annoy import AnnoyIndex\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import glob, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_LAYER_INDEX = -2 # default index into XCeption that will determine the image vector shape\n",
    "\n",
    "# VGG16, VGG19, and ResNet take 224×224 images; InceptionV3 and Xception take 299×299 inputs\n",
    "def resize_array(arr, shape=(299,299)):\n",
    "  '''Resize an array to a new shape'''\n",
    "  im = array_to_img(arr)\n",
    "  resized = im.resize(shape)\n",
    "  return img_to_array(resized)\n",
    "\n",
    "\n",
    "def fit_transform(obj, sample_layer_index=SAMPLE_LAYER_INDEX):\n",
    "  # input shape must be n_images, h, w, colors: https://keras.io/preprocessing/image/\n",
    "  if hasattr(obj, 'shape') and len(obj.shape) == 3:\n",
    "    # this is an array of a single image\n",
    "    arr = resize_array(obj)\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "  elif hasattr(obj, 'shape') and len(obj.shape) == 4:\n",
    "    # this is an array of images\n",
    "    arr = np.array([resize_array(i) for i in obj])\n",
    "  else:\n",
    "    # this is assumed to be a single image\n",
    "    arr = img_to_array(img.resize((299,299))) # see SO 47697622\n",
    "  # only Xception requires preprocessing\n",
    "  arr = imagenet_utils.preprocess_input(arr)\n",
    "  # extract the ith layer from the model (here, the -1th layer, or final layer)\n",
    "  out = K.function([model.input], [model.layers[sample_layer_index].output])([arr])\n",
    "  # parse out the k dim vector (k is determined by the size of the layer that's sampled from)\n",
    "  vec = out[0]\n",
    "  # return the vector to the calling scope\n",
    "  return vec\n",
    "\n",
    "\n",
    "model = Xception(weights='imagenet')\n",
    "setattr(model, 'fit_transform', fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model I/O Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(im, step=20, win=100, plot=False):\n",
    "  '''Given a np array `im` with at least two dimensions, return windows of len `win`'''\n",
    "  windows = []\n",
    "  dx = 0\n",
    "  dy = 0\n",
    "  for _ in range(im.shape[0]//step): # x axis pass\n",
    "    for _ in range(im.shape[1]//step): # y axis pass\n",
    "      dim = im[dy:dy+win, dx:dx+win]\n",
    "      if dim.shape == (win, win, 3):\n",
    "        windows.append(dim)\n",
    "      if plot:\n",
    "        plt.imshow(dim)\n",
    "        plt.title('{}-{}'.format(dx, dy))\n",
    "        plt.show()\n",
    "      dy += step\n",
    "    dx += step\n",
    "  return np.array(windows)\n",
    "\n",
    "def get_image_windows(img_path, color_mode='rgb'):\n",
    "  '''Given the path to an image, return an array of the image windows within that image'''\n",
    "  img = load_img(img_path, color_mode=color_mode)\n",
    "  arr = img_to_array(img)/255\n",
    "  return get_windows(arr)\n",
    "\n",
    "def get_image_vectors(img_path):\n",
    "  '''Given the path to an image, return an ndarray with shape (n_windows_in_img, 1000)'''\n",
    "  return model.fit_transform(get_image_windows(img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate feature separation given sample layer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from itertools import combinations\n",
    "from multiprocessing import Pool\n",
    "\n",
    "img_paths = glob.glob('utils/voynichese/images/*.jpg')\n",
    "im = img_paths[0]\n",
    "n = 40\n",
    "rows = int(n/10)\n",
    "columns = 10\n",
    "size_scalar = 4\n",
    "\n",
    "# get this image's windows\n",
    "windows = get_image_windows(im)\n",
    "\n",
    "def process_layer_index(layer_index):\n",
    "  print(' * processing layer', layer_index)\n",
    "  # get this image'svectors\n",
    "  vectors = model.fit_transform(windows, layer_index)\n",
    "\n",
    "  fig, axes = plt.subplots(rows, columns, figsize=(columns*size_scalar,rows*size_scalar))\n",
    "  for odx, o in enumerate(vectors[:n]):\n",
    "    o = o.flatten().squeeze()\n",
    "    column = odx%columns\n",
    "    row = odx//columns\n",
    "    axes[row][column].scatter(range(len(o)), o, s=0.01)\n",
    "  plt.savefig('layer-index-{}-scatter.png'.format(layer_index))\n",
    "  plt.clf()\n",
    "  \n",
    "  # get the histogram of vector similarities\n",
    "  distances = []\n",
    "  for i, j in combinations(vectors, 2):\n",
    "    i = i.flatten().squeeze()\n",
    "    j = j.flatten().squeeze()\n",
    "    d = scipy.spatial.distance.cosine(i, j)\n",
    "    distances.append(d)\n",
    "  plt.hist(distances, bins=50)\n",
    "  plt.savefig('layer-index-{}-hist.png'.format(layer_index))\n",
    "  plt.clf()\n",
    "  \n",
    "  # find the aggregate distance of all windows\n",
    "  d = np.sum(distances)\n",
    "  print(' * aggregate distance', layer_index, d)\n",
    "  print(' * dims size', layer_index, o.shape)\n",
    "  return [len(o), d]\n",
    "  \n",
    "if False:\n",
    "  distances = {}\n",
    "  shapes = {}\n",
    "  for i in range(1, 21, 1):\n",
    "    #i -= 20\n",
    "    try:\n",
    "      shape, distance = process_layer_index(i)\n",
    "      distances[i] = distance\n",
    "      shapes[i] = shape\n",
    "    except:\n",
    "      print(' * could not process idx', i)\n",
    "\n",
    "  with open('analysis-results.json', 'w') as out:\n",
    "    json.dump({\n",
    "      'distances': distances,\n",
    "      'shapes': shapes,\n",
    "    }, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback, os\n",
    "\n",
    "img_paths = glob.glob('utils/voynichese/images/*.jpg')\n",
    "color_mode = 'grayscale'\n",
    "\n",
    "out_dir = 'npy'\n",
    "if not os.path.exists(out_dir):\n",
    "  os.makedirs(out_dir)\n",
    "\n",
    "for idx, i in enumerate(img_paths[:3]):\n",
    "  try:\n",
    "    windows_path = os.path.join(out_dir, 'windows#' + i.replace('/', '#') + '#' + str(idx))\n",
    "    vectors_path = os.path.join(out_dir, 'vectors#' + i.replace('/', '#') + '#' + str(idx))\n",
    "    if not os.path.exists(windows_path + '.npy') or not os.path.exists(vectors_path + '.npy'):\n",
    "      print(' * processing image', idx+1)\n",
    "      windows = get_image_windows(i, color_mode=color_mode)\n",
    "      vectors = model.fit_transform(windows)\n",
    "      np.save(windows_path, windows)\n",
    "      np.save(vectors_path, vectors)\n",
    "  except Exception as exc:\n",
    "    print(' * could not parse', i, traceback.print_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Image Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_d[collection_name][image_idx][vec_idx] = array with shape (1,1000)\n",
    "# img_d[collection_name][image_idx][vec_idx] = array with shape (win, win, 3)\n",
    "vec_d = defaultdict(lambda: defaultdict(lambda: defaultdict()))\n",
    "img_d = defaultdict(lambda: defaultdict(lambda: defaultdict()))\n",
    "\n",
    "idx_to_meta = {}\n",
    "counter_idx = 0\n",
    "\n",
    "n_dims = 2048\n",
    "n_trees = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AnnoyIndex(n_dims, 'angular')\n",
    "for vector_path in glob.glob(os.path.join(out_dir, 'vectors*.npy')):\n",
    "  window_path = vector_path.replace('vectors#', 'windows#')\n",
    "  vectors = np.load(vector_path)\n",
    "  windows = np.load(window_path)\n",
    "  # determine the collection and path index for this path\n",
    "  s = os.path.basename(vector_path).split('#')\n",
    "  collection = '#'.join(s[:-1])\n",
    "  image_idx = int(s[-1].split('.')[0])\n",
    "  # process the collection - each vec in vectors is a window on the given page\n",
    "  for vec_idx, vec in enumerate(vectors):\n",
    "    idx_to_meta[counter_idx] = {'collection': collection, 'img_idx': image_idx, 'vec_idx': vec_idx}\n",
    "    vec_d[collection][image_idx][vec_idx] = vec.tolist()\n",
    "    img_d[collection][image_idx][vec_idx] = windows[vec_idx].tolist()\n",
    "    \n",
    "    t.add_item(counter_idx, vec_d[collection][image_idx][vec_idx])\n",
    "    counter_idx += 1\n",
    "    \n",
    "t.build(n_trees)\n",
    "t.save('voynich.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vec_d.json', 'w') as out: json.dump(vec_d, out)\n",
    "with open('img_d.json', 'w') as out: json.dump(img_d, out)\n",
    "with open('idx_to_meta.json', 'w') as out: json.dump(idx_to_meta, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_idx_to_vec(idx):\n",
    "  '''Given an image idx return an image array'''\n",
    "  m = idx_to_meta[idx]\n",
    "  return img_d[ m['collection'] ][ m['img_idx'] ][ m['vec_idx'] ]\n",
    "  \n",
    "def get_knn(idx, display=True, n=10):\n",
    "  im = image_idx_to_vec(idx)  \n",
    "  knn, distances = u.get_nns_by_item(idx, n, include_distances=True)\n",
    "  knn = knn[1:]\n",
    "  distances = distances[1:]\n",
    "  if display:\n",
    "    plt.imshow(im)\n",
    "    plt.show()\n",
    "    for i in knn:\n",
    "      im = image_idx_to_vec(i)\n",
    "      plt.imshow(im)\n",
    "      plt.show()\n",
    "  return knn, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = AnnoyIndex(n_dims, 'angular')\n",
    "u.load('voynich.ann') # super fast, will just mmap the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn, distances = get_knn(122, n=10, display=True)\n",
    "sims = [1-d for d in distances]\n",
    "sims"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
