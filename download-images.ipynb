{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "from bs4 import BeautifulSoup\n",
    "from copy import deepcopy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os, time, traceback, json, requests, shutil, glob, uuid\n",
    "\n",
    "# base metadata object to be updated on a per collection then per image basis\n",
    "base_meta = {\n",
    "  'image': '',\n",
    "  'collection': '',\n",
    "  'title': '',\n",
    "  'artist': '',\n",
    "  'date': '',\n",
    "  'text': '',\n",
    "  'location': '',\n",
    "  'permalink': '',\n",
    "}\n",
    "\n",
    "# helper to make images and metadata dir for a given collection\n",
    "def make_out_dirs(root_out_dir):\n",
    "  dirs = []\n",
    "  for i in ['images', 'metadata']:\n",
    "    out_dir = os.path.join('data', root_out_dir, i)\n",
    "    if not os.path.exists(out_dir):\n",
    "      os.makedirs(out_dir)\n",
    "    dirs.append(out_dir)\n",
    "  return dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Voynichese Images\n",
    "\n",
    "The following downloads the images and metadata for a dataset of inputs to the Neural Neighbors web application.\n",
    "\n",
    "Metadata for each image should ideally posess the following fields:\n",
    "\n",
    "| Image | Collection | Title | Artist | Date | Text | Location | Permalink |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import pages\n",
    "from os.path import join\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_from_url(url, out_path):\n",
    "  '''Download a file at location `url` and write to `out_path`'''\n",
    "  if not os.path.exists(out_path):\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(out_path, 'wb').write(r.content)\n",
    "\n",
    "def download_voynichese_data(page_ids):\n",
    "  '''Download page images and word coords from voynichese.com'''\n",
    "  for i in ['coords', 'images']:\n",
    "    out_dir = os.path.join('utils', 'voynichese', i)\n",
    "    if not os.path.exists(out_dir):\n",
    "      os.makedirs(out_dir)\n",
    "  for page_id in page_ids:\n",
    "    # download coords\n",
    "    url = 'http://www.voynichese.com/2/data/folio/script/' + page_id + '.js'\n",
    "    download_from_url(url, join('utils', 'voynichese', 'coords', page_id + '.json'))\n",
    "    # download page\n",
    "    url = 'http://www.voynichese.com/2/data/folio/image/glance/color/large/' + page_id + '.jpg'\n",
    "    download_from_url(url, join('utils', 'voynichese', 'images', page_id + '.jpg'))\n",
    "\n",
    "print(' * preparing to download', sorted(pages.keys()))\n",
    "download_voynichese_data(pages.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Biodiversity Heritage Library Images (#1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from os.path import join\n",
    "import flickr_api # use python 2 kernel\n",
    "import os\n",
    "\n",
    "images_dir, meta_dir = make_out_dirs('biodivlibrary')\n",
    "collection_meta = deepcopy(base_meta)\n",
    "collection_meta.update({\n",
    "  'collection': 'Biodiversity Heritage Library',\n",
    "  'permalink': 'https://www.flickr.com/photos/biodivlibrary/'\n",
    "})\n",
    "\n",
    "# configure api keys\n",
    "credential_path = os.path.join('utils', 'flickr.credentials')\n",
    "flickr_api.set_keys(api_key='a704ce9732b363a9caece2d65f7d041a', api_secret ='f3f5e1d5baaf4d38')\n",
    "if os.path.exists(credential_path):\n",
    "  flickr_api.set_auth_handler(credential_path)\n",
    "else:\n",
    "  a = flickr_api.auth.AuthHandler() # creates a new AuthHandler object\n",
    "  perms = 'read' # set the required permissions\n",
    "  url = a.get_authorization_url(perms)\n",
    "  print(url) # open the printed url in a browser and agree; paste verifier code in set_verifier function below\n",
    "  a.set_verifier('5b58510bb6f0641b')\n",
    "  flickr_api.set_auth_handler(a)\n",
    "  a.save(credential_path)\n",
    "  \n",
    "# run initial query\n",
    "user = flickr_api.Person.findByUserName('biodivlibrary')\n",
    "errored = []\n",
    "photos = user.getPhotos(page=2, perpage=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_num = 0\n",
    "while page_num < photos.info.pages:\n",
    "  print(' * page num', page_num)\n",
    "  page_photos = user.getPhotos(page=page_num, perpage=100)  \n",
    "  for i in page_photos:\n",
    "    try:\n",
    "      # save photo\n",
    "      out_path = join(images_dir, i.id)\n",
    "      print(' * saving', out_path)\n",
    "      i.save(out_path)\n",
    "      time.sleep(2)\n",
    "      # save meta\n",
    "      info = i.getInfo() # url\n",
    "      url = info.get('urls', {}).get('url', [])\n",
    "      url = url[0].get('text', collection_meta['permalink']) if url else collection_meta['permalink']\n",
    "      meta = deepcopy(collection_meta)\n",
    "      meta.update({\n",
    "        'image': out_path,\n",
    "        'permalink': url,\n",
    "        'title': info.get('title', ''),\n",
    "        'text': info.get('description', ''),\n",
    "        'tags': [i.text for i in info.get('tags', [])],\n",
    "        'views': info.get('views', ''),\n",
    "      })\n",
    "      out_path = join(meta_dir, i.id + '.json')\n",
    "      with open(out_path, 'w') as out:\n",
    "        json.dump(meta, out)\n",
    "    except Exception as exc:\n",
    "      print(' * could not save', i, '\\n', traceback.format_exc().splitlines())\n",
    "      errored.append(i)\n",
    "  page_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Smithsonian Images (#11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir, meta_dir = make_out_dirs('smithsonian-botany')\n",
    "collection_meta = deepcopy(base_meta)\n",
    "collection_meta.update({\n",
    "  'collection': 'Smithsonian Botany Collection',\n",
    "  'permalink': 'https://library.si.edu/topic/botany/images',\n",
    "})\n",
    "\n",
    "# get images\n",
    "page_number = 0\n",
    "while page_number != None:\n",
    "  print(' * fetching page', page_number)\n",
    "  r = requests.get('https://library.si.edu/topic/botany/images?page=' + str(page_number))\n",
    "  text = r.content.decode('utf8')\n",
    "  soup = BeautifulSoup(text)\n",
    "  imgs = soup.select('.bg-hover-highlight')\n",
    "  for idx, i in enumerate(imgs):\n",
    "    print('   * fetching image idx', idx, 'on page')\n",
    "    try:\n",
    "      # download image\n",
    "      src = i.find('img')['src']\n",
    "      image_id = os.path.basename(src).split('id=')[1]\n",
    "      r = requests.get('https://ids.si.edu/ids/deliveryService?id=' + image_id, allow_redirects=True)\n",
    "      out_path = os.path.join(images_dir, image_id + '.jpg')\n",
    "      open(out_path, 'wb').write(r.content)\n",
    "      # save metadata\n",
    "      href = 'https://library.si.edu' + i.find('a')['href']\n",
    "      r = requests.get(href)\n",
    "      soup = BeautifulSoup(r.text)\n",
    "      meta = deepcopy(collection_meta)\n",
    "      try:\n",
    "        title = soup.select('#goi-title')[0].get_text()\n",
    "      except Exception:\n",
    "        print(' * title not available', href)\n",
    "        title = ''\n",
    "      try:\n",
    "        copy = ''\n",
    "        t1 = soup.select('.views-field-field-original-caption')\n",
    "        t2 = soup.select('.views-label-field-citation')\n",
    "        for k in [t1, t2]:\n",
    "          if k:\n",
    "            copy += k[0].get_text() + ' '\n",
    "      except Exception:\n",
    "        print(' * text not available', href)\n",
    "        copy = ''\n",
    "      try:\n",
    "        tags = [j.get_text() for i in soup.select('.ig-icon-subjects') for j in i.select('a')]\n",
    "      except Exception:\n",
    "        print(' * tags not available', href)\n",
    "        tags = []\n",
    "      meta.update({\n",
    "        'image': out_path,\n",
    "        'title': ' '.join(title.split()).strip(),\n",
    "        'text': ' '.join(copy.split()).strip(),\n",
    "        'permalink': href,\n",
    "        'tags': tags,\n",
    "      })\n",
    "      out_path = os.path.join(meta_dir, image_id + '.json')\n",
    "      with open(out_path, 'w') as out:\n",
    "        json.dump(meta, out)\n",
    "      \n",
    "    except Exception as exc:\n",
    "      print(' ! err', i, exc)\n",
    "  if '/topic/botany/images?page=' + str(page_number+1) in text:\n",
    "    page_number += 1\n",
    "  else:\n",
    "    page_number = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Bodleian Selected Images (#9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images_dir, meta_dir = make_out_dirs('bodleian')\n",
    "collection_meta = deepcopy(base_meta)\n",
    "collection_meta.update({\n",
    "  'collection': 'Bodleian Herbals',\n",
    "  'permalink': 'https://digital.bodleian.ox.ac.uk/inquire/Discover/Search/#/?p=c+NaN,t+herbal,rsrs+0,rsps+100,fa+,so+ox%3Asort%5Easc,scids+,pid+,vi+',\n",
    "})\n",
    "\n",
    "def download_query_results():\n",
    "  driver = webdriver.Chrome()\n",
    "  driver.get(url)\n",
    "  time.sleep(3)\n",
    "  text = driver.page_source\n",
    "  soup = BeautifulSoup(text)\n",
    "  collections = driver.find_elements_by_css_selector('.result')\n",
    "  print(' * got', len(collections), 'collections')\n",
    "  for i in collections: # i = manuscript\n",
    "    # strange app\n",
    "    i.click()\n",
    "    time.sleep(2)\n",
    "    meta = deepcopy(collection_meta)\n",
    "    meta.update({\n",
    "      'title': soup.select('#metadata_title_text')[0].get_text(),\n",
    "      'description': ' '.join(soup.select('#metadata_shelfmark_text')[0].get_text().split()).strip(),\n",
    "      'text': ' '.join(soup.select('#metadata_shelfmark')[0].get_text().split()).strip(),\n",
    "    })\n",
    "    children = i.get_attribute('data-children').strip().split(',')\n",
    "    for j in children:\n",
    "      try:\n",
    "        r = requests.get('https://digital.bodleian.ox.ac.uk/inquire/resolver.iip?FIF={0}.jp2&HEI=514&RGN=0,0,1,1&CVT=jpeg'.format(j))\n",
    "        out_path = os.path.join(images_dir, j + '.jpg')\n",
    "        open(out_path, 'wb').write(r.content)\n",
    "        meta.update({\n",
    "          'image': out_path,\n",
    "        })\n",
    "        with open(os.path.join(meta_dir, j + '.json'), 'w') as out:\n",
    "          json.dump(meta, out)\n",
    "      except Exception as exc:\n",
    "        print(' ! err', j, exc)\n",
    "\n",
    "# get images\n",
    "root_url = 'https://digital.bodleian.ox.ac.uk/inquire/Discover/Search/#/?'\n",
    "url = root_url + 'p=c+NaN,t+herbal,rsrs+0,rsps+100,fa+,so+ox%3Asort%5Easc,scids+,pid+,vi+'\n",
    "\n",
    "download_query_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Padova Images (#19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "\n",
    "images_dir, meta_dir = make_out_dirs('padova')\n",
    "collection_meta = deepcopy(base_meta)\n",
    "collection_meta.update({\n",
    "  'collection': 'Padova',\n",
    "  'permalink': 'https://medicaltraditions.org/padova/images',\n",
    "  'title': 'Padova, Biblioteca del Seminario, 194',\n",
    "})\n",
    "\n",
    "for i in ['recto', 'verso']:\n",
    "  for j in range(1,28,1):\n",
    "    # download image\n",
    "    num = str(j)\n",
    "    while len(num) < 3:\n",
    "      num = '0' + num\n",
    "    img = '{0}-{1}.jpg'.format(num, i)\n",
    "    url = 'https://medicaltraditions.org/images/stories/manuscripts/demateriamedica/{0}'.format(img)\n",
    "    r = requests.get(url)\n",
    "    open(os.path.join(images_dir, img), 'wb').write(r.content)\n",
    "    # save meta\n",
    "    meta = deepcopy(collection_meta)\n",
    "    meta['image'] = img\n",
    "    with open(os.path.join(meta_dir, img.replace('.jpg', '.json')), 'w') as out:\n",
    "      json.dump(meta, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Penn Database Images (#36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir, meta_dir = make_out_dirs('penn-manuscripts')\n",
    "collection_meta = deepcopy(base_meta)\n",
    "collection_meta.update({\n",
    "  'text': 'Illustrated Herbal',\n",
    "  'collection': 'Penn Manuscripts',\n",
    "})\n",
    "\n",
    "for z in [1, 16, 31, 46, 61]:\n",
    "  text = requests.get('http://sceti.library.upenn.edu/ljs/ljsbrowse.cfm?StartRow={0}#'.format(z)).text\n",
    "  soup = BeautifulSoup(text)\n",
    "  for i in soup.select('#schoenlist')[0].select('tr.hioff'):\n",
    "    # only ms with images will have thumbnails in the first table column\n",
    "    thumb, book_id, title, date = [w.get_text().strip() for w in i.select('td')]\n",
    "    if not book_id: continue\n",
    "    thumb = i.select('td')[0]\n",
    "    if not thumb: continue\n",
    "    thumb_path = thumb.select('img')\n",
    "    if not thumb_path: continue\n",
    "    if 'ljsthumbs' not in thumb_path[0]['src']: continue\n",
    "    print(' * fetching', book_id)\n",
    "    meta = deepcopy(collection_meta)\n",
    "    meta['title'] = title\n",
    "    meta['date'] = date\n",
    "    meta['collection'] = book_id\n",
    "    for j in range(500):\n",
    "      jdx = str(j)\n",
    "      while len(jdx) < 4: jdx = '0' + jdx\n",
    "      for k in ['front', 'body']:\n",
    "        try:\n",
    "          # save images\n",
    "          img = '{0}_{1}{2}'.format(book_id, k, jdx)\n",
    "          url = 'http://images.library.upenn.edu/mrsidsceti/bin/image_jpeg.pl?coll=schoenberg&subcoll={0}&image={1}.sid&level=2'.format(book_id, img)\n",
    "          r = requests.get(url)\n",
    "          if len(r.content) > 1000:\n",
    "            out_path = os.path.join(images_dir, img + '.jpg')\n",
    "            open(out_path, 'wb').write(r.content)\n",
    "            print(' * saved', out_path)\n",
    "            # save metadata\n",
    "            meta.update({\n",
    "              'image': out_path,\n",
    "              'permalink': url,\n",
    "            })\n",
    "            out_path = os.path.join(meta_dir, img + '.json')\n",
    "            with open(out_path, 'w') as out:\n",
    "              json.dump(meta, out)\n",
    "        except:\n",
    "          print(' ! could not fetch page', url, traceback.format_exc().splitlines())\n",
    "    # many ms don't have images, so clear those out\n",
    "    if len(glob.glob(os.path.join(images_dir, '*'))) == 0:\n",
    "      shutil.rmtree(os.path.join('data', book_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bax - Italian Herbal (#33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir, meta_dir = make_out_dirs('uvm-italian-herbal')\n",
    "collection_meta = deepcopy(base_meta)\n",
    "collection_meta.update({\n",
    "  'collection': 'UVM Italian Herbal'\n",
    "})\n",
    "\n",
    "# system uses Islandora and given a list of thumbnails like:\n",
    "# http://cdi.uvm.edu/islandora/object/uvmcdi%3A55290/pages\n",
    "# one can transform each thumbnail url from\n",
    "# http://cdi.uvm.edu/islandora/object/uvmcdi%3A55304/datastream/TN/view\n",
    "# to another DATASTREAM in Islandora https://wiki.duraspace.org/display/ISLANDORA/APPENDIX+C+-+DATASTREAM+REFERENCE\n",
    "# e.g. http://cdi.uvm.edu/islandora/object/uvmcdi%3A55304/datastream/JPG/view\n",
    "for page_idx, page in enumerate(range(1, 12, 1)):\n",
    "  url = 'http://cdi.uvm.edu/islandora/object/uvmcdi%3A55290/pages?page={0}'.format(page)\n",
    "  html = requests.get(url).text\n",
    "  soup = BeautifulSoup(html)\n",
    "  for idx, i in enumerate(soup.select('.islandora-objects-grid')[0].select('img')):\n",
    "    # save image\n",
    "    src = i['src']\n",
    "    url = src.replace('/TN/', '/JPG/')\n",
    "    r = requests.get(url)\n",
    "    img = '{0}-{1}.jpg'.format(page_idx, idx)\n",
    "    open(os.path.join(images_dir, img), 'wb').write(r.content)\n",
    "    # save meta\n",
    "    meta = deepcopy(collection_meta)\n",
    "    meta.update({\n",
    "      'image': img,\n",
    "      'title': img,\n",
    "      'permalink': url,\n",
    "    })\n",
    "    with open(os.path.join(meta_dir, img.replace('.jpg', '.json')), 'w') as out:\n",
    "      json.dump(meta, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bax - General History of the Things of New Spain (#33)\n",
    "Author: Sahagun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "\n",
    "images_dir, meta_dir = make_out_dirs('sahagun')\n",
    "collection_meta = deepcopy(base_meta)\n",
    "collection_meta.update({\n",
    "  'collection': 'Sahagun - General History of the Things of New Spain',\n",
    "  'title': 'Sahagun - General History of the Things of New Spain',\n",
    "})\n",
    "\n",
    "for i in range(1000):\n",
    "  url = 'https://content.wdl.org/10622/service/thumbnail/1403114302/1024x1024/1/{0}.jpg'.format(i)\n",
    "  r = requests.get(url)\n",
    "  if len(r.content) > 500:\n",
    "    img = str(i) + '.jpg'\n",
    "    out_path = os.path.join(images_dir, img)\n",
    "    open(out_path, 'wb').write(r.content)\n",
    "    # save the metadata\n",
    "    meta = deepcopy(collection_meta)\n",
    "    meta['permalink'] = url\n",
    "    meta['image'] = out_path\n",
    "    with open(os.path.join(meta_dir, img.replace('.jpg', '.json')), 'w') as out:\n",
    "      json.dump(meta, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bax - Kitab (#33)\n",
    "Manuscrit Kitāb 'ağā'ib al-maḫlūqāt wa ġarā'ib ... Qazwīnī, Zakariyyā ibn Muḥammad ibn Maḥmūd al- (1203-1283). Auteur du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "\n",
    "images_dir, meta_dir = make_out_dirs('kitab')\n",
    "collection_meta = deepcopy(base_meta)\n",
    "collection_meta.update({\n",
    "  'collection': 'Kitab',\n",
    "  'title': 'Kitab',\n",
    "  'author': 'Qazwīnī, Zakariyyā ibn Muḥammad ibn',\n",
    "  'text': \"Ms. orné de figures coloriées, représentant les animaux et les plantes décrits dans le texte. Il est daté de l'an 1176 de l'hégire (1762-1763 de J. C.).\",\n",
    "})\n",
    "  \n",
    "for i in range(1000):\n",
    "  url = 'https://gallica.bnf.fr/ark:/12148/btv1b8406160j/f{0}.medres'.format(i)\n",
    "  r = requests.get(url)\n",
    "  if len(r.content) > 500:\n",
    "    # save image\n",
    "    img = str(i) + '.jpg'\n",
    "    out_path = os.path.join(images_dir, img)\n",
    "    open(out_path, 'wb').write(r.content)\n",
    "    # write meta\n",
    "    meta = deepcopy(collection_meta)\n",
    "    meta['permalink'] = url\n",
    "    meta['image'] = out_path\n",
    "    with open(os.path.join(meta_dir, img.replace('.jpg', '.json')), 'w') as out:\n",
    "      json.dump(meta, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS. Canon. Misc. 408 & Bodleian Medieval Manuscripts (#34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages: https://iiif.bodleian.ox.ac.uk/iiif/mirador/c444f7e2-ca30-48ae-87b5-54f93d6ed046\n",
    "# manifest: https://iiif.bodleian.ox.ac.uk/iiif/manifest/db2fade4-61ee-4a11-a894-19361c551eed.json\n",
    "# image: https://iiif.bodleian.ox.ac.uk/iiif/image/ca0cddf4-d66c-41a5-84b6-c72aee34ba65/full/600,/0/default.jpg\n",
    "# info https://iiif.bodleian.ox.ac.uk/iiif/image/c444f7e2-ca30-48ae-87b5-54f93d6ed046/info.json\n",
    "\n",
    "# from the manifest, find the first value in the sequences key, then on that page each page is listed\n",
    "# as the @id value in the canvases list\n",
    "\n",
    "# download all bodleian medieval manuscripts\n",
    "for idx, i in enumerate(range(1,5,1)): # page index\n",
    "  url = 'https://medieval.bodleian.ox.ac.uk/?f_inclusive%5Bms_digitized_s%5D%5B%5D=Yes&ms_title_s=&name_s=&op=AND&page={0}&per_page=100&search_field=advanced&sort=score+desc%2C+sort_title+asc'.format(i)\n",
    "  soup = BeautifulSoup(requests.get(url).text)\n",
    "  for jdx, j in enumerate(soup.select('#documents')[0].select('.document')): # manuscript index\n",
    "    href = 'https://medieval.bodleian.ox.ac.uk' + j.select('a')[0]['href']\n",
    "    page = BeautifulSoup(requests.get(href).text)\n",
    "    title = page.select('h1')[0].get_text()\n",
    "    # find the link to the manuscript images\n",
    "    href = page.select('.surrogates')[0].select('a')[0]['href']\n",
    "    book_id = href.split('/')[-1]\n",
    "    # make the output directories for this book\n",
    "    images_dir, meta_dir = make_out_dirs('bodleian')\n",
    "    # fetch the mirador page for this book\n",
    "    url = 'https://iiif.bodleian.ox.ac.uk/iiif/manifest/{0}.json'.format(book_id)\n",
    "    manifest = requests.get(url).json()\n",
    "    sequence_url = manifest['sequences'][0]['@id']\n",
    "    sequence = requests.get(sequence_url).json()\n",
    "    image_ids = [k['@id'].split('/')[-1].split('.')[0] for k in sequence['canvases']]\n",
    "    meta = deepcopy(base_meta)\n",
    "    meta.update({\n",
    "      'title': ''.join([k['value'] for k in manifest['metadata'] if k['label'] == 'Title']),\n",
    "      'text': ''.join([k['value'] for k in manifest['metadata'] if k['label'] == 'Additional Information']),\n",
    "      'date': ''.join([k['value'] for k in manifest['metadata'] if k['label'] == 'Date Statement']),\n",
    "      'collection': 'Bodleian Medieval Manuscripts',\n",
    "    })\n",
    "    for image_id in image_ids:\n",
    "      try:\n",
    "        # download image\n",
    "        image_url = 'https://iiif.bodleian.ox.ac.uk/iiif/image/{0}/full/600,/0/default.jpg'.format(image_id)\n",
    "        r = requests.get(image_url)\n",
    "        if (len(r.text) > 1000):\n",
    "          out_path = os.path.join(images_dir, image_id + '.jpg')\n",
    "          with open(out_path, 'wb') as out:\n",
    "            out.write(r.content)\n",
    "          # write metadata\n",
    "          meta.update({\n",
    "            'image': image_url,\n",
    "            'permalink': image_url,\n",
    "          })\n",
    "          out_path = os.path.join(meta_dir, image_id + '.json')\n",
    "          with open(out_path, 'w') as out:\n",
    "            json.dump(meta, out)\n",
    "      except:\n",
    "        print(' ! could not download image', image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hunt Images (#10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir, meta_dir = make_out_dirs('hunt')\n",
    "collection_meta = deepcopy(base_meta)\n",
    "collection_meta.update({\n",
    "  'collection': 'Hunt',\n",
    "  'author': '',\n",
    "  'text': '',\n",
    "})\n",
    "\n",
    "for i in range(1,185,1):\n",
    "  url = 'http://huntbot.org/artdb/art-collection-search?page={0}'.format(i)\n",
    "  html = requests.get(url).text\n",
    "  soup = BeautifulSoup(html)\n",
    "  for idx, j in enumerate(soup.select('.views-field img')):\n",
    "    try:\n",
    "      url = j['src']\n",
    "      r = requests.get(url)\n",
    "      if len(r.content) > 500:\n",
    "        # download image\n",
    "        img = '{0}-{1}.jpg'.format(i, idx)\n",
    "        out_path = os.path.join(images_dir, img)\n",
    "        open(out_path, 'wb').write(r.content)\n",
    "        # save meta\n",
    "        meta = deepcopy(collection_meta)\n",
    "        meta.update({\n",
    "          'image': out_path,\n",
    "          'title': 'Hunt-{0}'.format(i),\n",
    "          'permalink': url,\n",
    "        })\n",
    "        with open(os.path.join(meta_dir, img.replace('.jpg', '.json')), 'w') as out:\n",
    "          json.dump(meta, out)\n",
    "    except Exception as exc:\n",
    "      print(' ! could not download', url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morgan Images (#25)\n",
    "\n",
    "Good candidate for discussing data \"scraping\" with student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir, meta_dir = make_out_dirs('morgan')\n",
    "collection_meta = deepcopy(base_meta)\n",
    "collection_meta.update({\n",
    "  'collection': 'Morgan',\n",
    "})\n",
    "\n",
    "root_url = 'http://ica.themorgan.org/'\n",
    "for i in range(0,4,1): # page idx\n",
    "  url = 'https://www.themorgan.org/manuscripts/list?page={0}'.format(i)\n",
    "  html = requests.get(url).text\n",
    "  soup = BeautifulSoup(html)\n",
    "  for idx, j in enumerate(soup.select('.views-field-field-collection-images-link a')): # manuscript idx on page\n",
    "    url = j['href']\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    for kdx, k in enumerate(soup.select('.starter-template img')): # image idx in manuscript\n",
    "      url = root_url + k['src'].replace('../', '')\n",
    "      try:\n",
    "        r = requests.get(url)\n",
    "        if len(r.content) > 500:\n",
    "          # download image\n",
    "          img = '{0}-{1}.jpg'.format(idx, kdx)\n",
    "          out_path = os.path.join(images_dir, img)\n",
    "          open(out_path, 'wb').write(r.content)\n",
    "          # save metadata\n",
    "          meta = deepcopy(collection_meta)\n",
    "          meta.update({\n",
    "            'image': out_path,\n",
    "            'title': img,\n",
    "            'permalink': url,\n",
    "          })\n",
    "          with open(os.path.join(meta_dir, img.replace('.jpg', '.json')), 'w') as out:\n",
    "            json.dump(meta, out)\n",
    "          \n",
    "      except Exception as exc:\n",
    "        print(' ! trouble with', idx, kdx, traceback.format_exc().splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# British Library\n",
    "\n",
    "Has 700 medieval manuscripts in full color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images_dir, meta_dir = make_out_dirs('british-library')\n",
    "collection_meta = deepcopy(base_meta)\n",
    "collection_meta.update({\n",
    "  'collection': 'British Library',\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def get_soup(url, timeout=5):\n",
    "  driver.get(url)\n",
    "  time.sleep(2)\n",
    "  try:\n",
    "    text = text = driver.page_source\n",
    "    if '<title>429 Too Many Requests</title>' in text:\n",
    "      print(' * nginx timed out')\n",
    "      time.sleep(timeout)\n",
    "      return get_html(url, timeout=timeout+5)\n",
    "    return BeautifulSoup(text)\n",
    "  except Exception as exc:\n",
    "    print(' ! error parsing html', traceback.format_exc().splitlines())\n",
    "    return ''\n",
    "\n",
    "def get_image(url, timeout=5):\n",
    "  r = requests.get(url)\n",
    "  if '<title>429 Too Many Requests</title>' in r.text:\n",
    "    time.sleep(timeout)\n",
    "    return get_image(url, timeout=timeout+5)\n",
    "  return r.content\n",
    "  \n",
    "for idx, i in enumerate(range(1,79,1)): # page idx\n",
    "  url = 'https://www.bl.uk/collection-items?formats=manuscript&page={0}'.format(i)\n",
    "  soup = get_soup(url)\n",
    "  for jdx, j in enumerate(soup.select('.pnl-title a')): # manuscript idx\n",
    "    url = 'https://www.bl.uk' + j['href']\n",
    "    soup = get_soup(url)\n",
    "    try:\n",
    "      images_url = 'https://www.bl.uk' + soup.select('#view-image-button')[0]['href']\n",
    "    except:\n",
    "      print(' ! no image link', url)\n",
    "      continue\n",
    "    soup = get_soup(images_url)\n",
    "    try:\n",
    "      title = soup.find(id='full-title').get_text() # such a wonky api\n",
    "    except Exception as exc:\n",
    "      title = ''\n",
    "      print(' ! no title', url, exc, soup)\n",
    "    try:\n",
    "      text = soup.find_all('p')[1].get_text()\n",
    "    except Exception as exc:\n",
    "      text = ''\n",
    "      print(' ! no text', url, exc, soup)\n",
    "    \n",
    "    for kdx, k in enumerate(soup.select('.img-viewer-thumbs img')):\n",
    "      try:\n",
    "        tail = '?'.join(k['src'].split('?')[:-1]) + '?w=500' # get images at desired width\n",
    "        url = 'https://www.bl.uk' + tail\n",
    "        data = get_image(url)\n",
    "        # download image\n",
    "        img = '{0}-{1}-{2}.jpg'.format(idx, jdx, kdx)\n",
    "        out_path = os.path.join(images_dir, img)\n",
    "        open(out_path, 'wb').write(data)\n",
    "        # save metadata\n",
    "        meta = deepcopy(collection_meta)\n",
    "        meta.update({\n",
    "          'image': out_path,\n",
    "          'title': title,\n",
    "          'text': text,\n",
    "          'permalink': url,\n",
    "        })\n",
    "        with open(os.path.join(meta_dir, img.replace('.jpg', '.json')), 'w') as out:\n",
    "          json.dump(meta, out)\n",
    "      except Exception as exc:\n",
    "        print(' ! could not download image', url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
