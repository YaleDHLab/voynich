{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# base metadata object to be updated on a per collection then per image basis\n",
    "meta = {\n",
    "  'image': '',\n",
    "  'collection': '',\n",
    "  'title': '',\n",
    "  'artist': '',\n",
    "  'date': '',\n",
    "  'text': '',\n",
    "  'location': '',\n",
    "  'permalink': '',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Voynichese Images\n",
    "\n",
    "The following downloads the images and metadata for a dataset of inputs to the Neural Neighbors web application.\n",
    "\n",
    "Metadata for each image should ideally posess the following fields:\n",
    "\n",
    "| Image | Collection | Title | Artist | Date | Text | Location | Permalink |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from helpers import pages\n",
    "\n",
    "def download_from_url(url, out_path):\n",
    "  '''Download a file at location `url` and write to `out_path`'''\n",
    "  if not os.path.exists(out_path):\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(out_path, 'wb').write(r.content)\n",
    "\n",
    "def download_voynichese_coords(page_id):\n",
    "  '''Download the page coords for `page_id` from voynichese.com'''\n",
    "  url = 'http://www.voynichese.com/2/data/folio/script/' + page_id + '.js'\n",
    "  download_from_url(url, join('voynichese', 'coords', page_id + '.js'))\n",
    "\n",
    "def download_voynichese_page(page_id):\n",
    "  '''Download a page image with page id `page_id` from voynichese.com'''\n",
    "  url = 'http://www.voynichese.com/2/data/folio/image/glance/color/large/' + page_id + '.jpg'\n",
    "  download_from_url(url, join('voynichese', 'images', page_id + '.jpg'))\n",
    "\n",
    "def download_voynichese_data(page_ids):\n",
    "  '''Download page images and word coords from voynichese.com'''\n",
    "  for i in ['coords', 'images']:\n",
    "    if not os.path.exists(join('voynichese', i)):\n",
    "      os.makedirs(join('voynichese', i))\n",
    "  for page_id in page_ids:\n",
    "    download_voynichese_coords(page_id)\n",
    "    download_voynichese_page(page_id)\n",
    "\n",
    "print(' * preparing to download', sorted(pages.keys()))\n",
    "download_voynichese_data(pages.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Biodiversity Heritage Library Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from os.path import join\n",
    "import flickr_api # use python 2 kernel\n",
    "import os\n",
    "\n",
    "root_out_dir = 'biodivlibrary'\n",
    "for i in ['images', 'metadata']:\n",
    "  out_dir = os.path.join(root_out_dir, i)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "collection_meta = deepcopy(meta)\n",
    "collection_meta.update({\n",
    "  'collection': 'Biodiversity Heritage Library',\n",
    "  'permalink': 'https://www.flickr.com/photos/biodivlibrary/'\n",
    "})\n",
    "\n",
    "# configure api keys\n",
    "flickr_api.set_keys(api_key='a704ce9732b363a9caece2d65f7d041a', api_secret ='f3f5e1d5baaf4d38')\n",
    "if os.path.exists('flickr.credentials'):\n",
    "  flickr_api.set_auth_handler('flickr.credentials')\n",
    "else:\n",
    "  a = flickr_api.auth.AuthHandler() # creates a new AuthHandler object\n",
    "  perms = 'read' # set the required permissions\n",
    "  url = a.get_authorization_url(perms)\n",
    "  print(url) # open the printed url in a browser and agree; paste verifier code in xml response below\n",
    "  \n",
    "if not os.path.exists('flickr.credentials'):\n",
    "  a.set_verifier('5b58510bb6f0641b')\n",
    "  flickr_api.set_auth_handler(a)\n",
    "  a.save('flickr.credentials')\n",
    "  \n",
    "# run initial query\n",
    "user = flickr_api.Person.findByUserName('biodivlibrary')\n",
    "errored = []\n",
    "photos = user.getPhotos(page=2, perpage=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, traceback, json\n",
    "\n",
    "page_num = 0\n",
    "while page_num < photos.info.pages:\n",
    "  print(' * page num', page_num)\n",
    "  page_photos = user.getPhotos(page=page_num, perpage=100)  \n",
    "  for i in page_photos:\n",
    "    try:\n",
    "      # save photo\n",
    "      out_path = join(root_out_dir, 'images', i.id + '.jpg')\n",
    "      print(' * saving', out_path)\n",
    "      i.save(out_path)\n",
    "      time.sleep(2)\n",
    "      # save meta\n",
    "      info = i.getInfo() # url\n",
    "      url = info.get('urls', {}).get('url', [])\n",
    "      url = url[0].get('text', collection_meta['permalink']) if url else collection_meta['permalink']\n",
    "      meta = deepcopy(collection_meta)\n",
    "      meta.update({\n",
    "        'permalink': url,\n",
    "        'title': info.get('title', ''),\n",
    "        'description': info.get('description', ''),\n",
    "        'tags': [i.text for i in info.get('tags', [])],\n",
    "        'views': info.get('views', ''),\n",
    "      })\n",
    "      out_path = join(root_out_dir, 'metadata', i.id + '.json')\n",
    "      with open(out_path, 'w') as out:\n",
    "        json.dump(meta, out)\n",
    "      \n",
    "    except Exception as exc:\n",
    "      print(' * could not save', i, '\\n', traceback.format_exc().splitlines())\n",
    "      errored.append(i)\n",
    "  page_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Smithsonian Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def get_page_images(page_number=0):\n",
    "  '''Save each herbal image in page number `page_number` to disk'''\n",
    "  print(' * fetching page', page_number)\n",
    "  r = requests.get('https://library.si.edu/topic/botany/images?page=' + str(page_number))\n",
    "  text = r.content.decode('utf8')\n",
    "  soup = BeautifulSoup(text)\n",
    "  imgs = soup.select('.dams-image')\n",
    "  for i in imgs:\n",
    "    try:\n",
    "      src = i.find('img')['src']\n",
    "      image_id = os.path.basename(src).split('id=')[1]\n",
    "      download_image(image_id)\n",
    "    except Exception as exc:\n",
    "      print(' ! err', i, exc)\n",
    "  if '/topic/botany/images?page=' + str(page_number+1) in text:\n",
    "    get_page_images(page_number+1)\n",
    "  \n",
    "def download_image(_id):\n",
    "  '''Download an image by SI image id'''\n",
    "  r = requests.get('https://ids.si.edu/ids/deliveryService?id=' + _id, allow_redirects=True)\n",
    "  open(os.path.join(out_dir, _id + '.jpg'), 'wb').write(r.content)\n",
    "\n",
    "# make output directory\n",
    "out_dir = 'smithsonian-images'\n",
    "if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
    "\n",
    "# get images\n",
    "get_page_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Bodleian Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_query_results():\n",
    "  driver = webdriver.Chrome()\n",
    "  driver.get(url)\n",
    "  time.sleep(2)\n",
    "  text = driver.page_source\n",
    "  soup = BeautifulSoup(text)\n",
    "  collections = soup.select('.result')\n",
    "  print(' * got', len(collections), 'collections')\n",
    "  for i in collections:\n",
    "    children = i['data-children'].strip().split(',')\n",
    "    for j in children:\n",
    "      try:\n",
    "        download_image(j)\n",
    "      except Exception as exc:\n",
    "        print(' ! err', j, exc)\n",
    "    \n",
    "def download_image(_id):\n",
    "  print(' * downloading', _id)\n",
    "  r = requests.get('https://digital.bodleian.ox.ac.uk/inquire/resolver.iip?FIF={0}.jp2&HEI=514&RGN=0,0,1,1&CVT=jpeg'.format(_id))\n",
    "  open(os.path.join(out_dir, _id + '.jpg'), 'wb').write(r.content)\n",
    "  \n",
    "# make output directory\n",
    "out_dir = 'bodleian-images'\n",
    "if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
    "\n",
    "# get images\n",
    "root_url = 'https://digital.bodleian.ox.ac.uk/inquire/Discover/Search/#/?'\n",
    "url = root_url + 'p=c+NaN,t+herbal,rsrs+0,rsps+100,fa+,so+ox%3Asort%5Easc,scids+,pid+,vi+'\n",
    "\n",
    "download_query_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Padova Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "\n",
    "out_dir = 'padova'\n",
    "if not os.path.exists(out_dir):\n",
    "  os.makedirs(out_dir)\n",
    "\n",
    "for i in ['recto', 'verso']:\n",
    "  for j in range(1,28,1):\n",
    "    num = str(j)\n",
    "    while len(num) < 3:\n",
    "      num = '0' + num\n",
    "    img = '{0}-{1}.jpg'.format(num, i)\n",
    "    url = 'https://medicaltraditions.org/images/stories/manuscripts/demateriamedica/{0}'.format(img)\n",
    "    r = requests.get(url)\n",
    "    open(os.path.join(out_dir, img), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Schoenberg Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "\n",
    "for book_id in ['ljs419', 'ljs062']:\n",
    "  \n",
    "  out_dir = os.path.join('schoenberg', book_id)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "  \n",
    "  for i in range(500):\n",
    "    idx = str(i)\n",
    "    while len(idx) < 4: idx = '0' + idx\n",
    "    for j in ['front', 'body']:\n",
    "      try:\n",
    "        img = '{0}_{1}{2}'.format(book_id, j, idx)\n",
    "        url = 'http://images.library.upenn.edu/mrsidsceti/bin/image_jpeg.pl?coll=schoenberg&subcoll={0}&image={1}.sid&level=2'.format(book_id, img)\n",
    "        r = requests.get(url)\n",
    "        print(url, len(r.content))\n",
    "        if len(r.content) > 1000:\n",
    "          open(os.path.join(out_dir, img + '.jpg'), 'wb').write(r.content)\n",
    "      except:\n",
    "        print(' ! could not fetch page', url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bax - Italian Herbal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "out_dir = 'bax/italian-herbal/'\n",
    "if not os.path.exists(out_dir):\n",
    "  os.makedirs(out_dir)\n",
    "\n",
    "# system uses Islandora and given a list of thumbnails like:\n",
    "# http://cdi.uvm.edu/islandora/object/uvmcdi%3A55290/pages\n",
    "# one can transform each thumbnail url from\n",
    "# http://cdi.uvm.edu/islandora/object/uvmcdi%3A55304/datastream/TN/view\n",
    "# to another DATASTREAM in Islandora https://wiki.duraspace.org/display/ISLANDORA/APPENDIX+C+-+DATASTREAM+REFERENCE\n",
    "# e.g. http://cdi.uvm.edu/islandora/object/uvmcdi%3A55304/datastream/JPG/view\n",
    "for page_idx, page in enumerate(range(1, 12, 1)):\n",
    "  url = 'http://cdi.uvm.edu/islandora/object/uvmcdi%3A55290/pages?page={0}'.format(page)\n",
    "  html = requests.get(url).text\n",
    "  soup = BeautifulSoup(html)\n",
    "  for idx, i in enumerate(soup.select('.islandora-objects-grid')[0].select('img')):\n",
    "    src = i['src']\n",
    "    url = src.replace('/TN/', '/JPG/')\n",
    "    r = requests.get(url)\n",
    "    img = '{0}-{1}.jpg'.format(page_idx, idx)\n",
    "    open(os.path.join(out_dir, img), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bax - General History of the Things of New Spain\n",
    "Sahagun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "\n",
    "out_dir = 'bax/sahagun/'\n",
    "if not os.path.exists(out_dir):\n",
    "  os.makedirs(out_dir)\n",
    "  \n",
    "for i in range(1000):\n",
    "  url = 'https://content.wdl.org/10622/service/thumbnail/1403114302/1024x1024/1/{0}.jpg'.format(i)\n",
    "  r = requests.get(url)\n",
    "  if len(r.content) > 500:\n",
    "    img = str(i) + '.jpg'\n",
    "    open(os.path.join(out_dir, img), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bax - Kitab\n",
    "ManuscritKitāb 'ağā'ib al-maḫlūqāt wa ġarā'ib ... Qazwīnī, Zakariyyā ibn Muḥammad ibn Maḥmūd al- (1203-1283). Auteur du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "\n",
    "out_dir = 'bax/kitab/'\n",
    "if not os.path.exists(out_dir):\n",
    "  os.makedirs(out_dir)\n",
    "\n",
    "for i in range(1000):\n",
    "  url = 'https://gallica.bnf.fr/ark:/12148/btv1b8406160j/f{0}.medres'.format(i)\n",
    "  r = requests.get(url)\n",
    "  if len(r.content) > 500:\n",
    "    img = str(i) + '.jpg'\n",
    "    open(os.path.join(out_dir, img), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS. Canon. Misc. 408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url https://iiif.bodleian.ox.ac.uk/iiif/image/c444f7e2-ca30-48ae-87b5-54f93d6ed046/full/full/0/default.jpg\n",
    "# info https://iiif.bodleian.ox.ac.uk/iiif/image/c444f7e2-ca30-48ae-87b5-54f93d6ed046/info.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hunt Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os, requests\n",
    "\n",
    "out_dir = 'hunt'\n",
    "if not os.path.exists(out_dir):\n",
    "  os.makedirs(out_dir)\n",
    "\n",
    "for i in range(1,185,1):\n",
    "  url = 'http://huntbot.org/artdb/art-collection-search?page={0}'.format(i)\n",
    "  html = requests.get(url).text\n",
    "  soup = BeautifulSoup(html)\n",
    "  for idx, j in enumerate(soup.select('.views-field img')):\n",
    "    url = j['src']\n",
    "    r = requests.get(url)\n",
    "    if len(r.content) > 500:\n",
    "      img = '{0}-{1}.jpg'.format(i, idx)\n",
    "      open(os.path.join(out_dir, img), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morgan Images\n",
    "\n",
    "Good candidate for discussing data \"scraping\" with student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os, requests\n",
    "\n",
    "out_dir = 'morgan'\n",
    "if not os.path.exists(out_dir):\n",
    "  os.makedirs(out_dir)\n",
    "\n",
    "root_url = 'http://ica.themorgan.org/'\n",
    "for i in range(0,4,1): # page idx\n",
    "  url = 'https://www.themorgan.org/manuscripts/list?page={0}'.format(i)\n",
    "  html = requests.get(url).text\n",
    "  soup = BeautifulSoup(html)\n",
    "  for idx, j in enumerate(soup.select('.views-field-field-collection-images-link a')): # manuscript idx on page\n",
    "    url = j['href']\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    for kdx, k in enumerate(soup.select('.starter-template img')): # image idx in manuscript\n",
    "      url = root_url + k['src'].replace('../', '')\n",
    "      try:\n",
    "        r = requests.get(url)\n",
    "        if len(r.content) > 500:\n",
    "          img = '{0}-{1}.jpg'.format(idx, kdx)\n",
    "          open(os.path.join(out_dir, img), 'wb').write(r.content)\n",
    "      except Exception as exc:\n",
    "        print(' ! trouble with', idx, kdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
